{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f32e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparktorch import serialize_torch_obj, SparkTorch\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469ec9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbb913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54636e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bbb5813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/13 23:16:13 WARN Utils: Your hostname, JUANs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.10.30 instead (on interface en0)\n",
      "22/06/13 23:16:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/06/13 23:16:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\n",
    "    'local[4]'\n",
    ").config(\n",
    "    \"spark.driver.memory\",'2g'\n",
    ").config(\n",
    "    \"spark.executor.memory\",'1g'\n",
    ").appName('abc').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d8a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = '../data/04_model_input/train'\n",
    "cv = '../data/04_model_input/cv'\n",
    "test = '../data/04_model_input/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9567264",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.parquet(train)\n",
    "cv_df = spark.read.parquet(cv)\n",
    "test_df = spark.read.parquet(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86d53de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_join(df):\n",
    "    df = df.drop('address','number')\n",
    "    df_1 = df.select(*(col(x).alias(x + '_x') for x in df.columns)).withColumn('join', lit(1))\n",
    "    df_2 = df.select(*(col(x).alias(x + '_y') for x in df.columns)).withColumn('join', lit(1))\n",
    "    return df_1.join(\n",
    "        df_2, on='join'\n",
    "    ).withColumn(\n",
    "        'same',(col('name_x')==col('name_y')).astype(IntegerType())\n",
    "    ).drop('join','name_x','name_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31e7c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_join = inner_join(train_df.sample(False,0.001))\n",
    "cv_join = inner_join(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae701dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/13 23:17:59 WARN DAGScheduler: Broadcasting large task binary with size 1117.5 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_join.write.parquet(train+'_join')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f0ba6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_cols = [c for c in train_join.columns if c !='same']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf0bbd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_assembler = VectorAssembler(inputCols=in_cols,outputCol='features',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05cff107",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_join = vector_assembler.transform(train_join).select('features','same')\n",
    "cv_join = vector_assembler.transform(cv_join).select('features','same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2847f204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, same: int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6383a00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nn.Sequential(\n",
    "    nn.Linear(len(in_cols), 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 5),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e2dc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_obj = serialize_torch_obj(\n",
    "    model=network,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    lr=0.0001\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c95cd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_model = SparkTorch(\n",
    "    inputCol='features',\n",
    "    labelCol='same',\n",
    "    predictionCol='predictions',\n",
    "    torchObj=torch_obj,\n",
    "    iters=50,\n",
    "    verbose=1,\n",
    "    miniBatch=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea4aa037",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/13 23:04:28 WARN DAGScheduler: Creating new stage failed due to exception - job: 3\n",
      "org.apache.spark.scheduler.BarrierJobUnsupportedRDDChainException: [SPARK-24820][SPARK-24821]: Barrier execution mode does not allow the following pattern of RDD chain within a barrier stage:\n",
      "1. Ancestor RDDs that have different number of partitions from the resulting RDD (e.g. union()/coalesce()/first()/take()/PartitionPruningRDD). A workaround for first()/take() can be barrierRdd.collect().head (scala) or barrierRdd.collect()[0] (python).\n",
      "2. An RDD that depends on multiple barrier RDDs (e.g. barrierRdd1.zip(barrierRdd2)).\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.checkBarrierStageWithRDDChainPattern(DAGScheduler.scala:447)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:590)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1196)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2592)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.scheduler.BarrierJobUnsupportedRDDChainException: [SPARK-24820][SPARK-24821]: Barrier execution mode does not allow the following pattern of RDD chain within a barrier stage:\n1. Ancestor RDDs that have different number of partitions from the resulting RDD (e.g. union()/coalesce()/first()/take()/PartitionPruningRDD). A workaround for first()/take() can be barrierRdd.collect().head (scala) or barrierRdd.collect()[0] (python).\n2. An RDD that depends on multiple barrier RDDs (e.g. barrierRdd1.zip(barrierRdd2)).\n\tat org.apache.spark.scheduler.DAGScheduler.checkBarrierStageWithRDDChainPattern(DAGScheduler.scala:447)\n\tat org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:590)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1196)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2592)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_join\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/face_pairing/lib/python3.10/site-packages/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/miniforge3/envs/face_pairing/lib/python3.10/site-packages/sparktorch/torch_distributed.py:297\u001b[0m, in \u001b[0;36mSparkTorch._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m master_url \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39mgetConf()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.host\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m()\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynchronous\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 297\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_distributed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_shuffles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmini_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_pct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_pct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_patience\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhogwild\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m barrier:\n",
      "File \u001b[0;32m~/miniforge3/envs/face_pairing/lib/python3.10/site-packages/sparktorch/distributed.py:267\u001b[0m, in \u001b[0;36mtrain_distributed\u001b[0;34m(rdd, torch_obj, iters, partition_shuffles, verbose, mini_batch, validation_pct, world_size, device, early_stop_patience)\u001b[0m\n\u001b[1;32m    249\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(partition_shuffles):\n\u001b[1;32m    251\u001b[0m \n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# Run model with barrier execution mode.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmapPartitionsWithIndex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtorch_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_loaded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmaster_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaster_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m            \u001b[49m\u001b[43miters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmini_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmini_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_pct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_pct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m            \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m partition_shuffles \u001b[38;5;241m-\u001b[39m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    270\u001b[0m         num_partitions \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mgetNumPartitions()\n",
      "File \u001b[0;32m~/miniforge3/envs/face_pairing/lib/python3.10/site-packages/pyspark/rdd.py:950\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 950\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/miniforge3/envs/face_pairing/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/miniforge3/envs/face_pairing/lib/python3.10/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniforge3/envs/face_pairing/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.scheduler.BarrierJobUnsupportedRDDChainException: [SPARK-24820][SPARK-24821]: Barrier execution mode does not allow the following pattern of RDD chain within a barrier stage:\n1. Ancestor RDDs that have different number of partitions from the resulting RDD (e.g. union()/coalesce()/first()/take()/PartitionPruningRDD). A workaround for first()/take() can be barrierRdd.collect().head (scala) or barrierRdd.collect()[0] (python).\n2. An RDD that depends on multiple barrier RDDs (e.g. barrierRdd1.zip(barrierRdd2)).\n\tat org.apache.spark.scheduler.DAGScheduler.checkBarrierStageWithRDDChainPattern(DAGScheduler.scala:447)\n\tat org.apache.spark.scheduler.DAGScheduler.createResultStage(DAGScheduler.scala:590)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1196)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2592)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "spark_model.fit(train_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4402353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3f7088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_faces = keras.Input(shape = (len(in_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d70a2467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-13 22:03:35.927074: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-06-13 22:03:35.927386: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "dense_1 = layers.Dense(20, activation='relu')(input_faces)\n",
    "predictions = layers.Dense(1, activation='sigmoid')(dense_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f09f79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=input_faces, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28c4445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  'adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38c7ab8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_join' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_join\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_join' is not defined"
     ]
    }
   ],
   "source": [
    "train_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "900cf0e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'slice_X' from 'keras.models' (/Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages/keras/models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_from_yaml, slice_X\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'slice_X' from 'keras.models' (/Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages/keras/models/__init__.py)"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_yaml, slice_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0970250",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'slice_X' from 'keras.models' (/Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages/keras/models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melephas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspark_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkModel\n",
      "File \u001b[0;32m~/miniforge3/envs/face_pairing/lib/python3.10/site-packages/elephas/spark_model.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib2\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matrix, Vector\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_from_yaml, slice_X\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrwlock\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RWLock\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m subtract_params\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'slice_X' from 'keras.models' (/Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages/keras/models/__init__.py)"
     ]
    }
   ],
   "source": [
    "from elephas.spark_model import SparkModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcaa950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800e539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "spark_model = SparkModel(model, frequency='epoch', mode='asynchronous')\n",
    "spark_model.fit(rdd, epochs=20, batch_size=32, verbose=0, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb44a2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'keyword_only' from 'pyspark.ml.util' (/Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages/pyspark/ml/util.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melephas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ElephasEstimator\n",
      "File \u001b[0;32m~/miniforge3/envs/face_pairing/lib/python3.10/site-packages/elephas/ml_model.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparam\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshared\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HasInputCol, HasOutputCol, HasFeaturesCol, HasLabelCol\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keyword_only\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Row\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Estimator, Model\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'keyword_only' from 'pyspark.ml.util' (/Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages/pyspark/ml/util.py)"
     ]
    }
   ],
   "source": [
    "from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbdb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.util import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c79d816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import elephas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8b0d68d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'elephas' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43melephas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'elephas' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "elephas.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e2559f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'slice_X' from 'keras.models' (/Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages/keras/models/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melephas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdd_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_simple_rdd\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melephas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspark_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkModel\n",
      "File \u001b[0;32m~/miniforge3/envs/face_pairing/lib/python3.10/site-packages/elephas/spark_model.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib2\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matrix, Vector\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_from_yaml, slice_X\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrwlock\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RWLock\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m subtract_params\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'slice_X' from 'keras.models' (/Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages/keras/models/__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from elephas.utils.rdd_utils import to_simple_rdd\n",
    "from elephas.spark_model import SparkModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd89775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elephas in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (0.3)\n",
      "Collecting elephas\n",
      "  Using cached elephas-3.1.0.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cython\n",
      "  Using cached Cython-0.29.30-py2.py3-none-any.whl (985 kB)\n",
      "Collecting elephas\n",
      "  Using cached elephas-3.0.0.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-2.1.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-2.0.0.tar.gz (26 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-1.4.3.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-1.4.2.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-1.4.1.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-1.3.1.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-1.2.1.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-1.2.0.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-1.1.0.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-1.0.0.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-0.4.5.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-0.4.4.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Using cached elephas-0.4.3-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: keras in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from elephas) (2.9.0)\n",
      "Requirement already satisfied: hyperas in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from elephas) (0.4.1)\n",
      "Requirement already satisfied: flask in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from elephas) (2.1.2)\n",
      "Requirement already satisfied: pyspark in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from elephas) (3.2.1)\n",
      "  Using cached elephas-0.4.2-py3-none-any.whl (34 kB)\n",
      "  Using cached elephas-0.4.1-py3-none-any.whl (34 kB)\n",
      "  Using cached elephas-0.4-py3-none-any.whl (33 kB)\n",
      "Collecting pydl4j>=0.1.3\n",
      "  Using cached pydl4j-0.1.5-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: entrypoints in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperas->elephas) (0.4)\n",
      "Requirement already satisfied: nbformat in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperas->elephas) (5.4.0)\n",
      "Requirement already satisfied: hyperopt in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperas->elephas) (0.2.7)\n",
      "Requirement already satisfied: nbconvert in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperas->elephas) (6.4.5)\n",
      "Requirement already satisfied: jupyter in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperas->elephas) (1.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperopt->hyperas->elephas) (4.64.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperopt->hyperas->elephas) (2.1.0)\n",
      "Requirement already satisfied: py4j in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperopt->hyperas->elephas) (0.10.9.3)\n",
      "Requirement already satisfied: networkx>=2.2 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperopt->hyperas->elephas) (2.8.3)\n",
      "Requirement already satisfied: scipy in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperopt->hyperas->elephas) (1.8.1)\n",
      "Requirement already satisfied: future in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperopt->hyperas->elephas) (0.18.2)\n",
      "Requirement already satisfied: numpy in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperopt->hyperas->elephas) (1.22.4)\n",
      "Requirement already satisfied: six in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from hyperopt->hyperas->elephas) (1.15.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from jupyter->hyperas->elephas) (7.7.0)\n",
      "Requirement already satisfied: notebook in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from jupyter->hyperas->elephas) (6.4.12)\n",
      "Requirement already satisfied: ipykernel in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from jupyter->hyperas->elephas) (6.13.1)\n",
      "Requirement already satisfied: qtconsole in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from jupyter->hyperas->elephas) (5.3.1)\n",
      "Requirement already satisfied: jupyter-console in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from jupyter->hyperas->elephas) (6.4.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (4.11.1)\n",
      "Requirement already satisfied: testpath in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (0.6.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (0.2.2)\n",
      "Requirement already satisfied: bleach in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (5.0.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (2.12.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (0.8.4)\n",
      "Requirement already satisfied: jinja2>=2.4 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (3.1.2)\n",
      "Requirement already satisfied: traitlets>=5.0 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (5.2.2.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (2.1.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (0.5.13)\n",
      "Requirement already satisfied: jupyter-core in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (4.10.0)\n",
      "Requirement already satisfied: defusedxml in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbconvert->hyperas->elephas) (0.7.1)\n",
      "Requirement already satisfied: fastjsonschema in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbformat->hyperas->elephas) (2.15.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbformat->hyperas->elephas) (4.6.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->hyperas->elephas) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->hyperas->elephas) (0.18.1)\n",
      "Requirement already satisfied: nest-asyncio in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->hyperas->elephas) (1.5.5)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->hyperas->elephas) (7.3.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soupsieve>1.2 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from beautifulsoup4->nbconvert->hyperas->elephas) (2.3.1)\n",
      "Requirement already satisfied: webencodings in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from bleach->nbconvert->hyperas->elephas) (0.5.1)\n",
      "Requirement already satisfied: packaging in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipykernel->jupyter->hyperas->elephas) (21.3)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipykernel->jupyter->hyperas->elephas) (6.1)\n",
      "Requirement already satisfied: debugpy>=1.0 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipykernel->jupyter->hyperas->elephas) (1.6.0)\n",
      "Requirement already satisfied: psutil in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipykernel->jupyter->hyperas->elephas) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipykernel->jupyter->hyperas->elephas) (0.1.3)\n",
      "Requirement already satisfied: appnope in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipykernel->jupyter->hyperas->elephas) (0.1.3)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipykernel->jupyter->hyperas->elephas) (8.4.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipywidgets->jupyter->hyperas->elephas) (3.6.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipywidgets->jupyter->hyperas->elephas) (1.1.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipywidgets->jupyter->hyperas->elephas) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from jupyter-console->jupyter->hyperas->elephas) (3.0.29)\n",
      "Requirement already satisfied: pyzmq>=17 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from notebook->jupyter->hyperas->elephas) (23.1.0)\n",
      "Requirement already satisfied: argon2-cffi in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from notebook->jupyter->hyperas->elephas) (21.3.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from notebook->jupyter->hyperas->elephas) (0.15.0)\n",
      "Requirement already satisfied: prometheus-client in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from notebook->jupyter->hyperas->elephas) (0.14.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from notebook->jupyter->hyperas->elephas) (1.8.0)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from qtconsole->jupyter->hyperas->elephas) (2.1.0)\n",
      "Requirement already satisfied: decorator in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->hyperas->elephas) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->hyperas->elephas) (4.8.0)\n",
      "Requirement already satisfied: stack-data in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->hyperas->elephas) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->hyperas->elephas) (0.7.5)\n",
      "Requirement already satisfied: backcall in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->hyperas->elephas) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->hyperas->elephas) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel->jupyter->hyperas->elephas) (62.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert->hyperas->elephas) (2.8.2)\n",
      "Requirement already satisfied: wcwidth in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->hyperas->elephas) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from terminado>=0.8.3->notebook->jupyter->hyperas->elephas) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from argon2-cffi->notebook->jupyter->hyperas->elephas) (21.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from packaging->ipykernel->jupyter->hyperas->elephas) (3.0.9)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->hyperas->elephas) (0.8.3)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->hyperas->elephas) (1.15.0)\n",
      "Requirement already satisfied: asttokens in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->hyperas->elephas) (2.0.5)\n",
      "Requirement already satisfied: executing in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->hyperas->elephas) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter->hyperas->elephas) (0.2.2)\n",
      "Requirement already satisfied: pycparser in /Users/juanazuero/miniforge3/envs/face_pairing/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->hyperas->elephas) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade elephas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ea1b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
